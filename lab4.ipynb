{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Demonstrate how can you apply the PorterStemmer for finding the stem of word “Understanding” and“Demonstration”! \n",
    "\n",
    "it remove the suffix  by th rule = ((m > 1) S1 -> S2\n",
    "m = measure of any word\n",
    "\n",
    "(m>0)ATION -> ATE   Demonstration -> Demonstrate\n",
    "(m>1) ATE->\t\t    Demonstrate\t  -> Demonstr \n",
    "\n",
    "-\n",
    "\n",
    "(m>1) ing->\t\t    Understanding -> Understand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.stem import PorterStemmer\n",
    "Stemmerporter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'understand'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"understanding\"\n",
    "Stemmerporter.stem(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'demonstr'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"demonstration\"\n",
    "Stemmerporter.stem(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Assume you have a list of words that you want to get their stem, write python script that find stem of each word in the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['understanding', 'demonstration']\n"
     ]
    }
   ],
   "source": [
    "list=[\"understanding\", \"demonstration\"]\n",
    "print(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "understand\n",
      "demonstr\n"
     ]
    }
   ],
   "source": [
    "for i in list:\n",
    "    \n",
    "    print(Stemmerporter.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs            -> dog\n",
      "programming     -> program\n",
      "programs        -> program\n",
      "programmed      -> program\n",
      "cakes           -> cake\n",
      "indices         -> indic\n",
      "matrices        -> matric\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aziz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "porter = PorterStemmer()\n",
    "l_words = ['dogs', 'programming', 'programs', 'programmed', 'cakes', 'indices', 'matrices']\n",
    "for word in l_words:\n",
    "    print(f'{word} \\t -> {porter.stem(word)}'.expandtabs(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Now, apply Poster Stemmer on the following sentence: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'stemmer', 'for', 'English', 'operating', 'on', 'the', 'stem', 'cat', 'should', 'identify', 'such', 'strings', 'as', 'cats', ',', 'catlike', ',', 'and', 'catty.', 'A', 'stemming', 'algorithm', 'might', 'also', 'reduce', 'the', 'words', 'fishing', ',', 'fished', ',', 'and', 'fisher', 'to', 'the', 'stem', 'fish.', 'The', 'stem', 'need', 'not', 'be', 'a', 'word', ',', 'for', 'example', 'the', 'Porter', 'algorithm', 'reduces', ',', 'argue', ',', 'argued', ',', 'argues', ',', 'arguing', ',', 'and', 'argus', 'to', 'the', 'stem', 'argu', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "sentence = 'A stemmer for English operating on the stem cat should identify such strings as cats, catlike, and catty. A stemming algorithm might also reduce the words fishing, fished, and fisher to the stem fish. The stem need not be a word, for example the Porter algorithm reduces, argue, argued, argues, arguing, and argus to the stem argu.'\n",
    "#textt = nltk.word_tokenize(sentence) \n",
    "textt= tokenizer.tokenize(sentence)\n",
    "print(textt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrices        -> A\n",
      "matrices        -> stemmer\n",
      "matrices        -> for\n",
      "matrices        -> english\n",
      "matrices        -> oper\n",
      "matrices        -> on\n",
      "matrices        -> the\n",
      "matrices        -> stem\n",
      "matrices        -> cat\n",
      "matrices        -> should\n",
      "matrices        -> identifi\n",
      "matrices        -> such\n",
      "matrices        -> string\n",
      "matrices        -> as\n",
      "matrices        -> cat\n",
      "matrices        -> ,\n",
      "matrices        -> catlik\n",
      "matrices        -> ,\n",
      "matrices        -> and\n",
      "matrices        -> catty.\n",
      "matrices        -> A\n",
      "matrices        -> stem\n",
      "matrices        -> algorithm\n",
      "matrices        -> might\n",
      "matrices        -> also\n",
      "matrices        -> reduc\n",
      "matrices        -> the\n",
      "matrices        -> word\n",
      "matrices        -> fish\n",
      "matrices        -> ,\n",
      "matrices        -> fish\n",
      "matrices        -> ,\n",
      "matrices        -> and\n",
      "matrices        -> fisher\n",
      "matrices        -> to\n",
      "matrices        -> the\n",
      "matrices        -> stem\n",
      "matrices        -> fish.\n",
      "matrices        -> the\n",
      "matrices        -> stem\n",
      "matrices        -> need\n",
      "matrices        -> not\n",
      "matrices        -> be\n",
      "matrices        -> a\n",
      "matrices        -> word\n",
      "matrices        -> ,\n",
      "matrices        -> for\n",
      "matrices        -> exampl\n",
      "matrices        -> the\n",
      "matrices        -> porter\n",
      "matrices        -> algorithm\n",
      "matrices        -> reduc\n",
      "matrices        -> ,\n",
      "matrices        -> argu\n",
      "matrices        -> ,\n",
      "matrices        -> argu\n",
      "matrices        -> ,\n",
      "matrices        -> argu\n",
      "matrices        -> ,\n",
      "matrices        -> argu\n",
      "matrices        -> ,\n",
      "matrices        -> and\n",
      "matrices        -> argu\n",
      "matrices        -> to\n",
      "matrices        -> the\n",
      "matrices        -> stem\n",
      "matrices        -> argu\n",
      "matrices        -> .\n"
     ]
    }
   ],
   "source": [
    "for i in textt:\n",
    "    \n",
    "    print(f'{word} \\t -> {porter.stem(i)}'.expandtabs(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.1: Repeat the task presented in exercise 2. Then,compare outputs! Do you find a difference between Porter Stemmer outputs and Lancaster Stemmer? Why??? \n",
    "Yes, porter is the less offensive algorithm because of this it broadly applied the. Also its the oldest stemming algorithm. The word roots are somewhat intuitive and obvious. But for the Lancaster algorithm, it is highly offensive due to the strictly cutting words and presenting it much confusing. by using the Lancaster algorithm, stems will become non-relatable to some degree, because of this its less practiced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrices        -> a\n",
      "matrices        -> stem\n",
      "matrices        -> for\n",
      "matrices        -> engl\n",
      "matrices        -> op\n",
      "matrices        -> on\n",
      "matrices        -> the\n",
      "matrices        -> stem\n",
      "matrices        -> cat\n",
      "matrices        -> should\n",
      "matrices        -> ident\n",
      "matrices        -> such\n",
      "matrices        -> strings\n",
      "matrices        -> as\n",
      "matrices        -> cat\n",
      "matrices        -> ,\n",
      "matrices        -> catlik\n",
      "matrices        -> ,\n",
      "matrices        -> and\n",
      "matrices        -> catty.\n",
      "matrices        -> a\n",
      "matrices        -> stem\n",
      "matrices        -> algorithm\n",
      "matrices        -> might\n",
      "matrices        -> also\n",
      "matrices        -> reduc\n",
      "matrices        -> the\n",
      "matrices        -> word\n",
      "matrices        -> fish\n",
      "matrices        -> ,\n",
      "matrices        -> fish\n",
      "matrices        -> ,\n",
      "matrices        -> and\n",
      "matrices        -> fish\n",
      "matrices        -> to\n",
      "matrices        -> the\n",
      "matrices        -> stem\n",
      "matrices        -> fish.\n",
      "matrices        -> the\n",
      "matrices        -> stem\n",
      "matrices        -> nee\n",
      "matrices        -> not\n",
      "matrices        -> be\n",
      "matrices        -> a\n",
      "matrices        -> word\n",
      "matrices        -> ,\n",
      "matrices        -> for\n",
      "matrices        -> exampl\n",
      "matrices        -> the\n",
      "matrices        -> port\n",
      "matrices        -> algorithm\n",
      "matrices        -> reduc\n",
      "matrices        -> ,\n",
      "matrices        -> argu\n",
      "matrices        -> ,\n",
      "matrices        -> argu\n",
      "matrices        -> ,\n",
      "matrices        -> argu\n",
      "matrices        -> ,\n",
      "matrices        -> argu\n",
      "matrices        -> ,\n",
      "matrices        -> and\n",
      "matrices        -> arg\n",
      "matrices        -> to\n",
      "matrices        -> the\n",
      "matrices        -> stem\n",
      "matrices        -> argu\n",
      "matrices        -> .\n"
     ]
    }
   ],
   "source": [
    "text=\"understanding\"\n",
    "from nltk.stem import LancasterStemmer\n",
    "StemmerLancaster = LancasterStemmer() \n",
    "StemmerLancaster.stem(text)\n",
    "for i in textt:\n",
    "    \n",
    "    print(f'{word} \\t -> {StemmerLancaster.stem(i)}'.expandtabs(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Check how to perform stemming using SnowballStemmer and Write your python script below! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "have\n",
      "having\n",
      "generous\n",
      "gener\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#print(\" \".join(SnowballStemmer.languages))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "print(stemmer.stem(\"running\"))\n",
    "stemmer2 = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "print(stemmer.stem(\"having\"))\n",
    "print(stemmer2.stem(\"having\"))\n",
    "print(SnowballStemmer(\"english\").stem(\"generously\"))\n",
    "print(SnowballStemmer(\"porter\").stem(\"generously\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "print(SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming in other Languages \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "حرك\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.isri import ISRIStemmer\n",
    "st = ISRIStemmer()\n",
    "w = 'حركات' \n",
    "print(st.stem(w)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It iterates through all the words in the text, if the word already exists in the\n",
      "corpus, it increments the frequency count for the word\n",
      "Stemmed sentence\n",
      "It iter through all the word in the text , if the word alreadi exist in the corpu , it increment the frequenc count for the word \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "file=open(r'C:\\Users\\Aziz\\Desktop\\file.txt', encoding=\"utf-8\")\n",
    "Sentences= file.read()\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words: \n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "print(Sentences)\n",
    "print(\"Stemmed sentence\")\n",
    "x=stemSentence(Sentences)\n",
    "print(x) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Download the WordNet corpora from NLTK downloader before using the WordNet Lemmatizer. Now Rune the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Aziz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 ha                  \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What did you note (up)?  \n",
    "There is something wrong with the stem for example has should be have but its ha. it misses appropriate labeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    " for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\"))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5. Now compare the output? What did you see? Why?  (up)\n",
    "I see that  the steming now is correct, becaues now we are using an appropriate labeling or appropriate POS tag method. pos stand for parts-of-speech and its a job of labeling each word in a sentence with its proper part of speech. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Task Exercise 6. Demonstrate how to use ISRIStemmer for stemming Arabic document. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سلم\n",
      "سلم\n",
      "حرك\n",
      "جمع\n",
      "درس\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "st = ISRIStemmer()\n",
    "file=open(r'C:\\Users\\Aziz\\Desktop\\file1.txt', encoding=\"utf-8\")\n",
    "Sentences= file.read()\n",
    "textt= tokenizer.tokenize(Sentences)\n",
    "for i in textt:\n",
    "    print(st.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
