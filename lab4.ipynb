{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.stem import PorterStemmer\n",
    "Stemmerporter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'understand'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"understanding\"\n",
    "Stemmerporter.stem(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'demonstr'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"demonstration\"\n",
    "Stemmerporter.stem(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['understanding', 'demonstration']\n"
     ]
    }
   ],
   "source": [
    "list=[\"understanding\", \"demonstration\"]\n",
    "print(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "understand\n",
      "demonstr\n"
     ]
    }
   ],
   "source": [
    "for i in list:\n",
    "    \n",
    "    print(Stemmerporter.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs            -> dog\n",
      "programming     -> program\n",
      "programs        -> program\n",
      "programmed      -> program\n",
      "cakes           -> cake\n",
      "indices         -> indic\n",
      "matrices        -> matric\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aziz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "porter = PorterStemmer()\n",
    "l_words = ['dogs', 'programming', 'programs', 'programmed', 'cakes', 'indices', 'matrices']\n",
    "for word in l_words:\n",
    "    print(f'{word} \\t -> {porter.stem(word)}'.expandtabs(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'stemmer', 'for', 'English', 'operating', 'on', 'the', 'stem', 'cat', 'should', 'identify', 'such', 'strings', 'as', 'cats', ',', 'catlike', ',', 'and', 'catty.', 'A', 'stemming', 'algorithm', 'might', 'also', 'reduce', 'the', 'words', 'fishing', ',', 'fished', ',', 'and', 'fisher', 'to', 'the', 'stem', 'fish.', 'The', 'stem', 'need', 'not', 'be', 'a', 'word', ',', 'for', 'example', 'the', 'Porter', 'algorithm', 'reduces', ',', 'argue', ',', 'argued', ',', 'argues', ',', 'arguing', ',', 'and', 'argus', 'to', 'the', 'stem', 'argu', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "sentence = 'A stemmer for English operating on the stem cat should identify such strings as cats, catlike, and catty. A stemming algorithm might also reduce the words fishing, fished, and fisher to the stem fish. The stem need not be a word, for example the Porter algorithm reduces, argue, argued, argues, arguing, and argus to the stem argu.'\n",
    "#textt = nltk.word_tokenize(sentence) \n",
    "textt= tokenizer.tokenize(sentence)\n",
    "print(textt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "stemmer\n",
      "for\n",
      "english\n",
      "oper\n",
      "on\n",
      "the\n",
      "stem\n",
      "cat\n",
      "should\n",
      "identifi\n",
      "such\n",
      "string\n",
      "as\n",
      "cat\n",
      ",\n",
      "catlik\n",
      ",\n",
      "and\n",
      "catty.\n",
      "A\n",
      "stem\n",
      "algorithm\n",
      "might\n",
      "also\n",
      "reduc\n",
      "the\n",
      "word\n",
      "fish\n",
      ",\n",
      "fish\n",
      ",\n",
      "and\n",
      "fisher\n",
      "to\n",
      "the\n",
      "stem\n",
      "fish.\n",
      "the\n",
      "stem\n",
      "need\n",
      "not\n",
      "be\n",
      "a\n",
      "word\n",
      ",\n",
      "for\n",
      "exampl\n",
      "the\n",
      "porter\n",
      "algorithm\n",
      "reduc\n",
      ",\n",
      "argu\n",
      ",\n",
      "argu\n",
      ",\n",
      "argu\n",
      ",\n",
      "argu\n",
      ",\n",
      "and\n",
      "argu\n",
      "to\n",
      "the\n",
      "stem\n",
      "argu\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in textt:\n",
    "    \n",
    "    print(porter.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "stem\n",
      "for\n",
      "engl\n",
      "op\n",
      "on\n",
      "the\n",
      "stem\n",
      "cat\n",
      "should\n",
      "ident\n",
      "such\n",
      "strings\n",
      "as\n",
      "cat\n",
      ",\n",
      "catlik\n",
      ",\n",
      "and\n",
      "catty.\n",
      "a\n",
      "stem\n",
      "algorithm\n",
      "might\n",
      "also\n",
      "reduc\n",
      "the\n",
      "word\n",
      "fish\n",
      ",\n",
      "fish\n",
      ",\n",
      "and\n",
      "fish\n",
      "to\n",
      "the\n",
      "stem\n",
      "fish.\n",
      "the\n",
      "stem\n",
      "nee\n",
      "not\n",
      "be\n",
      "a\n",
      "word\n",
      ",\n",
      "for\n",
      "exampl\n",
      "the\n",
      "port\n",
      "algorithm\n",
      "reduc\n",
      ",\n",
      "argu\n",
      ",\n",
      "argu\n",
      ",\n",
      "argu\n",
      ",\n",
      "argu\n",
      ",\n",
      "and\n",
      "arg\n",
      "to\n",
      "the\n",
      "stem\n",
      "argu\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "text=\"understanding\"\n",
    "from nltk.stem import LancasterStemmer\n",
    "StemmerLancaster = LancasterStemmer() \n",
    "StemmerLancaster.stem(text)\n",
    "for i in textt:\n",
    "    \n",
    "    print(StemmerLancaster.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "have\n",
      "having\n",
      "generous\n",
      "gener\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#print(\" \".join(SnowballStemmer.languages))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "print(stemmer.stem(\"running\"))\n",
    "stemmer2 = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "print(stemmer.stem(\"having\"))\n",
    "print(stemmer2.stem(\"having\"))\n",
    "print(SnowballStemmer(\"english\").stem(\"generously\"))\n",
    "print(SnowballStemmer(\"porter\").stem(\"generously\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "print(SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "حرك\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.isri import ISRIStemmer\n",
    "st = ISRIStemmer()\n",
    "w = 'حركات' \n",
    "print(st.stem(w)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It iterates through all the words in the text, if the word already exists in the\n",
      "corpus, it increments the frequency count for the word\n",
      "Stemmed sentence\n",
      "It iter through all the word in the text , if the word alreadi exist in the corpu , it increment the frequenc count for the word \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "file=open(r'C:\\Users\\Aziz\\Desktop\\file.txt', encoding=\"utf-8\")\n",
    "Sentences= file.read()\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words: \n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "print(Sentences)\n",
    "print(\"Stemmed sentence\")\n",
    "x=stemSentence(Sentences)\n",
    "print(x) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Aziz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 ha                  \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\"))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He\n",
      "was\n",
      "running\n",
      "and\n",
      "eating\n",
      "at\n",
      "same\n",
      "time.\n",
      "He\n",
      "has\n",
      "bad\n",
      "habit\n",
      "of\n",
      "swimming\n",
      "after\n",
      "playing\n",
      "long\n",
      "hours\n",
      "in\n",
      "the\n",
      "Sun\n",
      ".\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سلم\n",
      "سلم\n",
      "حرك\n",
      "جمع\n",
      "درس\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "st = ISRIStemmer()\n",
    "file=open(r'C:\\Users\\Aziz\\Desktop\\file1.txt', encoding=\"utf-8\")\n",
    "Sentences= file.read()\n",
    "textt= tokenizer.tokenize(Sentences)\n",
    "for i in textt:\n",
    "    print(st.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
